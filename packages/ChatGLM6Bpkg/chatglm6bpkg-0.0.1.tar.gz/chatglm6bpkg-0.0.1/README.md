# ChatGLM6Bpkg

è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main)å°è£…çš„åŒ…ï¼Œé€šè¿‡ChatGLM6Bpkgï¼Œç”¨æˆ·å¯ä»¥ç®€ä¾¿åœ°ä½¿ç”¨[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main)æ”¯æŒçš„å¤šç§åŠŸèƒ½ã€‚

## ä½¿ç”¨æ–¹å¼

### ç¡¬ä»¶éœ€æ±‚

| **é‡åŒ–ç­‰çº§**  | **æœ€ä½ GPU æ˜¾å­˜**ï¼ˆæ¨ç†ï¼‰ | **æœ€ä½ GPU æ˜¾å­˜**ï¼ˆé«˜æ•ˆå‚æ•°å¾®è°ƒï¼‰ |
| --------- | ----------------- | --------------------- |
| FP16ï¼ˆæ— é‡åŒ–ï¼‰ | 13 GB             | 14 GB                 |
| INT8      | 8 GB              | 9 GB                  |
| INT4      | 6 GB              | 7 GB                  |
### æ¨¡å‹åŠ è½½ 

å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š

```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
>>> response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
>>> print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:

1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
```
æ¨¡å‹çš„å®ç°ä»ç„¶å¤„åœ¨å˜åŠ¨ä¸­ã€‚å¦‚æœå¸Œæœ›å›ºå®šä½¿ç”¨çš„æ¨¡å‹å®ç°ä»¥ä¿è¯å…¼å®¹æ€§ï¼Œå¯ä»¥åœ¨ `from_pretrained` çš„è°ƒç”¨ä¸­å¢åŠ  `revision="v1.1.0"` å‚æ•°ã€‚`v1.1.0` æ˜¯å½“å‰æœ€æ–°çš„ç‰ˆæœ¬å·ï¼Œå®Œæ•´çš„ç‰ˆæœ¬åˆ—è¡¨å‚è§ [Change Log](https://huggingface.co/THUDM/chatglm-6b#change-log)ã€‚

### ä»æœ¬åœ°åŠ è½½æ¨¡å‹
ä»¥ä¸Šä»£ç ä¼šç”± `transformers` è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å®ç°å’Œå‚æ•°ã€‚å®Œæ•´çš„æ¨¡å‹å®ç°å¯ä»¥åœ¨ [Hugging Face Hub](https://huggingface.co/THUDM/chatglm-6b)ã€‚å¦‚æœä½ çš„ç½‘ç»œç¯å¢ƒè¾ƒå·®ï¼Œä¸‹è½½æ¨¡å‹å‚æ•°å¯èƒ½ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´ç”šè‡³å¤±è´¥ã€‚æ­¤æ—¶å¯ä»¥å…ˆå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶åä»æœ¬åœ°åŠ è½½ã€‚

ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹éœ€è¦å…ˆ[å®‰è£…Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)ï¼Œç„¶åè¿è¡Œ
```Shell
git clone https://huggingface.co/THUDM/chatglm-6b
```

å¦‚æœä½ ä» Hugging Face Hub ä¸Šä¸‹è½½ checkpoint çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯ä»¥åªä¸‹è½½æ¨¡å‹å®ç°
```Shell
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b
```
ç„¶åä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/)æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶ï¼Œå¹¶å°†ä¸‹è½½çš„æ–‡ä»¶æ›¿æ¢åˆ°æœ¬åœ°çš„ `chatglm-6b` ç›®å½•ä¸‹ã€‚

å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åï¼Œå°†ä»¥ä¸Šä»£ç ä¸­çš„ `THUDM/chatglm-6b` æ›¿æ¢ä¸ºä½ æœ¬åœ°çš„ `chatglm-6b` æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œå³å¯ä»æœ¬åœ°åŠ è½½æ¨¡å‹ã€‚

**Optional** æ¨¡å‹çš„å®ç°ä»ç„¶å¤„åœ¨å˜åŠ¨ä¸­ã€‚å¦‚æœå¸Œæœ›å›ºå®šä½¿ç”¨çš„æ¨¡å‹å®ç°ä»¥ä¿è¯å…¼å®¹æ€§ï¼Œå¯ä»¥æ‰§è¡Œ
```Shell
git checkout v1.1.0
```

## Demo & API

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº [Gradio](https://gradio.app) çš„ç½‘é¡µç‰ˆ Demo å’Œä¸€ä¸ªå‘½ä»¤è¡Œ Demoã€‚

### ç½‘é¡µç‰ˆ Demo

![web-demo](resources/web-demo.gif)

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š 

```python
import ChatGLM6Bpkg

model_name_or_path = "THUDM/chatglm-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half().cuda()
model = model.eval()
ChatGLM6Bpkg.launch_web_demo(model=model, tokenizer=tokenizer)
```

ç¨‹åºä¼šè¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚æœ€æ–°ç‰ˆ Demo å®ç°äº†æ‰“å­—æœºæ•ˆæœï¼Œé€Ÿåº¦ä½“éªŒå¤§å¤§æå‡ã€‚æ³¨æ„ï¼Œç”±äºå›½å†… Gradio çš„ç½‘ç»œè®¿é—®è¾ƒä¸ºç¼“æ…¢ï¼Œå¯ç”¨ `demo.queue().launch(share=True, inbrowser=True)` æ—¶æ‰€æœ‰ç½‘ç»œä¼šç»è¿‡ Gradio æœåŠ¡å™¨è½¬å‘ï¼Œå¯¼è‡´æ‰“å­—æœºä½“éªŒå¤§å¹…ä¸‹é™ï¼Œç°åœ¨é»˜è®¤å¯åŠ¨æ–¹å¼å·²ç»æ”¹ä¸º `share=False`ï¼Œå¦‚æœ‰éœ€è¦å…¬ç½‘è®¿é—®çš„éœ€æ±‚ï¼Œå¯ä»¥é‡æ–°ä¿®æ”¹ä¸º `share=True` å¯åŠ¨ã€‚

### å‘½ä»¤è¡Œ Demo

![cli-demo](resources/cli-demo.png)

ç¤ºä¾‹ä»£ç å¦‚ä¸‹

```python
import ChatGLM6Bpkg

model_name_or_path = "THUDM/chatglm-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half().cuda()
model = model.eval()
ChatGLM6Bpkg.launch_cli_demo(model=model, tokenizer=tokenizer)
```

ç¨‹åºä¼šåœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ `clear` å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ `stop` ç»ˆæ­¢ç¨‹åºã€‚

### APIéƒ¨ç½²
ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
import ChatGLM6Bpkg

model_name_or_path = "THUDM/chatglm-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half().cuda()
model = model.eval()
ChatGLM6Bpkg.launch_server(model=model, tokenizer=tokenizer)
```
é»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8000 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨
```shell
curl -X POST "http://127.0.0.1:8000" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½", "history": []}'
```
å¾—åˆ°çš„è¿”å›å€¼ä¸º
```shell
{
  "response":"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
  "history":[["ä½ å¥½","ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],
  "status":200,
  "time":"2023-03-23 21:38:40"
}
```

## ä½æˆæœ¬éƒ¨ç½²
### æ¨¡å‹é‡åŒ–
é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 13GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
# æŒ‰éœ€ä¿®æ”¹ï¼Œç›®å‰åªæ”¯æŒ 4/8 bit é‡åŒ–
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).quantize(8).half().cuda()
```

è¿›è¡Œ 2 è‡³ 3 è½®å¯¹è¯åï¼Œ8-bit é‡åŒ–ä¸‹ GPU æ˜¾å­˜å ç”¨çº¦ä¸º 10GBï¼Œ4-bit é‡åŒ–ä¸‹ä»…éœ€ 6GB å ç”¨ã€‚éšç€å¯¹è¯è½®æ•°çš„å¢å¤šï¼Œå¯¹åº”æ¶ˆè€—æ˜¾å­˜ä¹Ÿéšä¹‹å¢é•¿ï¼Œç”±äºé‡‡ç”¨äº†ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œç†è®ºä¸Š ChatGLM-6B æ”¯æŒæ— é™é•¿çš„ context-lengthï¼Œä½†æ€»é•¿åº¦è¶…è¿‡ 2048ï¼ˆè®­ç»ƒé•¿åº¦ï¼‰åæ€§èƒ½ä¼šé€æ¸ä¸‹é™ã€‚

æ¨¡å‹é‡åŒ–ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½æŸå¤±ï¼Œç»è¿‡æµ‹è¯•ï¼ŒChatGLM-6B åœ¨ 4-bit é‡åŒ–ä¸‹ä»ç„¶èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„ç”Ÿæˆã€‚ä½¿ç”¨ [GPT-Q](https://arxiv.org/abs/2210.17323) ç­‰é‡åŒ–æ–¹æ¡ˆå¯ä»¥è¿›ä¸€æ­¥å‹ç¼©é‡åŒ–ç²¾åº¦/æå‡ç›¸åŒé‡åŒ–ç²¾åº¦ä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œæ¬¢è¿å¤§å®¶æå‡ºå¯¹åº”çš„ Pull Requestã€‚

é‡åŒ–è¿‡ç¨‹éœ€è¦åœ¨å†…å­˜ä¸­é¦–å…ˆåŠ è½½ FP16 æ ¼å¼çš„æ¨¡å‹ï¼Œæ¶ˆè€—å¤§æ¦‚ 13GB çš„å†…å­˜ã€‚å¦‚æœä½ çš„å†…å­˜ä¸è¶³çš„è¯ï¼Œå¯ä»¥ç›´æ¥åŠ è½½é‡åŒ–åçš„æ¨¡å‹ï¼ŒINT4 é‡åŒ–åçš„æ¨¡å‹ä»…éœ€å¤§æ¦‚ 5.2GB çš„å†…å­˜ï¼š
```python
# INT8 é‡åŒ–çš„æ¨¡å‹å°†"THUDM/chatglm-6b-int4"æ”¹ä¸º"THUDM/chatglm-6b-int8"
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()
```
é‡åŒ–æ¨¡å‹çš„å‚æ•°æ–‡ä»¶ä¹Ÿå¯ä»¥ä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)æ‰‹åŠ¨ä¸‹è½½ã€‚

### CPU éƒ¨ç½²
å¦‚æœä½ æ²¡æœ‰ GPU ç¡¬ä»¶çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½†æ˜¯æ¨ç†é€Ÿåº¦ä¼šæ›´æ…¢ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼ˆéœ€è¦å¤§æ¦‚ 32GB å†…å­˜ï¼‰
```python
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).float()
```

å¦‚æœä½ çš„å†…å­˜ä¸è¶³ï¼Œå¯ä»¥ç›´æ¥åŠ è½½é‡åŒ–åçš„æ¨¡å‹ï¼š
```python
# INT8 é‡åŒ–çš„æ¨¡å‹å°†"THUDM/chatglm-6b-int4"æ”¹ä¸º"THUDM/chatglm-6b-int8"
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4",trust_remote_code=True).float()
```

å¦‚æœé‡åˆ°äº†æŠ¥é”™ `Could not find module 'nvcuda.dll'` æˆ–è€… `RuntimeError: Unknown platform: darwin` (MacOS) ï¼Œè¯·[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)

### Mac éƒ¨ç½²
å¯¹äºæ­è½½äº† Apple Silicon æˆ–è€… AMD GPU çš„Macï¼Œå¯ä»¥ä½¿ç”¨ MPS åç«¯æ¥åœ¨ GPU ä¸Šè¿è¡Œ ChatGLM-6Bã€‚éœ€è¦å‚è€ƒ Apple çš„ [å®˜æ–¹è¯´æ˜](https://developer.apple.com/metal/pytorch) å®‰è£… PyTorch-Nightlyï¼ˆæ­£ç¡®çš„ç‰ˆæœ¬å·åº”è¯¥æ˜¯2.1.0.dev2023xxxxï¼Œè€Œä¸æ˜¯2.0.0ï¼‰ã€‚

ç›®å‰åœ¨ MacOS ä¸Šåªæ”¯æŒ[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)ã€‚å°†ä»£ç ä¸­çš„æ¨¡å‹åŠ è½½æ”¹ä¸ºä»æœ¬åœ°åŠ è½½ï¼Œå¹¶ä½¿ç”¨ mps åç«¯ï¼š
```python
model = AutoModel.from_pretrained("your local path", trust_remote_code=True).half().to('mps')
```

åŠ è½½åŠç²¾åº¦çš„ ChatGLM-6B æ¨¡å‹éœ€è¦å¤§æ¦‚ 13GB å†…å­˜ã€‚å†…å­˜è¾ƒå°çš„æœºå™¨ï¼ˆæ¯”å¦‚ 16GB å†…å­˜çš„ MacBook Proï¼‰ï¼Œåœ¨ç©ºä½™å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹ä¼šä½¿ç”¨ç¡¬ç›˜ä¸Šçš„è™šæ‹Ÿå†…å­˜ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ä¸¥é‡å˜æ…¢ã€‚æ­¤æ—¶å¯ä»¥ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹å¦‚ chatglm-6b-int4ã€‚å› ä¸º GPU ä¸Šé‡åŒ–çš„ kernel æ˜¯ä½¿ç”¨ CUDA ç¼–å†™çš„ï¼Œå› æ­¤æ— æ³•åœ¨ MacOS ä¸Šä½¿ç”¨ï¼Œåªèƒ½ä½¿ç”¨ CPU è¿›è¡Œæ¨ç†ã€‚
```python
# INT8 é‡åŒ–çš„æ¨¡å‹å°†"THUDM/chatglm-6b-int4"æ”¹ä¸º"THUDM/chatglm-6b-int8"
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4",trust_remote_code=True).float()
```
ä¸ºäº†å……åˆ†ä½¿ç”¨ CPU å¹¶è¡Œï¼Œè¿˜éœ€è¦[å•ç‹¬å®‰è£… OpenMP](FAQ.md#q1)ã€‚

### å¤šå¡éƒ¨ç½²
å¦‚æœä½ æœ‰å¤šå¼  GPUï¼Œä½†æ˜¯æ¯å¼  GPU çš„æ˜¾å­˜å¤§å°éƒ½ä¸è¶³ä»¥å®¹çº³å®Œæ•´çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥å°†æ¨¡å‹åˆ‡åˆ†åœ¨å¤šå¼ GPUä¸Šã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
import ChatGLM6Bpkg

model = ChatGLM6Bpkg.load_model_on_gpus("THUDM/chatglm-6b", num_gpus=2)
```
å³å¯å°†æ¨¡å‹éƒ¨ç½²åˆ°ä¸¤å¼  GPU ä¸Šè¿›è¡Œæ¨ç†ã€‚ä½ å¯ä»¥å°† `num_gpus` æ”¹ä¸ºä½ å¸Œæœ›ä½¿ç”¨çš„ GPU æ•°ã€‚é»˜è®¤æ˜¯å‡åŒ€åˆ‡åˆ†çš„ï¼Œä½ ä¹Ÿå¯ä»¥ä¼ å…¥ `device_map` å‚æ•°æ¥è‡ªå·±æŒ‡å®šã€‚ 

## é«˜æ•ˆå‚æ•°å¾®è°ƒ
æœ¬é¡¹ç›®å®ç°äº†å¯¹äº ChatGLM-6B æ¨¡å‹åŸºäº [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) çš„å¾®è°ƒã€‚P-Tuning v2 å°†éœ€è¦å¾®è°ƒçš„å‚æ•°é‡å‡å°‘åˆ°åŸæ¥çš„ 0.1%ï¼Œå†é€šè¿‡æ¨¡å‹é‡åŒ–ã€Gradient Checkpoint ç­‰æ–¹æ³•ï¼Œæœ€ä½åªéœ€è¦ 7GB æ˜¾å­˜å³å¯è¿è¡Œã€‚

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•ã€‚

### ä¸‹è½½æ•°æ®é›†

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

```json
{
    "content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³",
    "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"
}
```

ä» [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) æˆ–è€… [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œå°†è§£å‹åçš„ `AdvertiseGen` ç›®å½•æ”¾åˆ°å½“å‰ç›®å½•ä¸‹ã€‚

### è®­ç»ƒ

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
import ChatGLM6Bpkg

ChatGLM6Bpkg.ptuning.ptuning(
    do_train=True,
    train_file="AdvertiseGen/train.json",
    validation_file="AdvertiseGen/dev.json",
    prompt_column="content",
    response_column="summary",
    overwrite_cache=True,
    model_name_or_path="THUDM/chatglm-6b",
    output_dir="output/adgen-chatglm-6b-pt-128-2e-2",
    overwrite_output_dir=True,
    max_source_length=64,
    max_target_length=64,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    predict_with_generate=True,
    max_steps=3000,
    logging_steps=10,
    save_steps=100,
    learning_rate=2e-2,
    pre_seq_len=128,
    quantization_bit=4
)
```

`pre_seq_len` å’Œ `learning_rate` åˆ†åˆ«æ˜¯ soft prompt é•¿åº¦å’Œè®­ç»ƒçš„å­¦ä¹ ç‡ï¼Œå¯ä»¥è¿›è¡Œè°ƒèŠ‚ä»¥å–å¾—æœ€ä½³çš„æ•ˆæœã€‚P-Tuning-v2 æ–¹æ³•ä¼šå†»ç»“å…¨éƒ¨çš„æ¨¡å‹å‚æ•°ï¼Œå¯é€šè¿‡è°ƒæ•´ `quantization_bit` æ¥è¢«åŸå§‹æ¨¡å‹çš„é‡åŒ–ç­‰çº§ï¼Œä¸åŠ æ­¤é€‰é¡¹åˆ™ä¸º FP16 ç²¾åº¦åŠ è½½ã€‚

åœ¨é»˜è®¤é…ç½® `quantization_bit=4`ã€`per_device_train_batch_size=1`ã€`gradient_accumulation_steps=16` ä¸‹ï¼ŒINT4 çš„æ¨¡å‹å‚æ•°è¢«å†»ç»“ï¼Œä¸€æ¬¡è®­ç»ƒè¿­ä»£ä¼šä»¥ 1 çš„æ‰¹å¤„ç†å¤§å°è¿›è¡Œ 16 æ¬¡ç´¯åŠ çš„å‰åå‘ä¼ æ’­ï¼Œç­‰æ•ˆä¸º 16 çš„æ€»æ‰¹å¤„ç†å¤§å°ï¼Œæ­¤æ—¶æœ€ä½åªéœ€ 6.7G æ˜¾å­˜ã€‚è‹¥æƒ³åœ¨åŒç­‰æ‰¹å¤„ç†å¤§å°ä¸‹æå‡è®­ç»ƒæ•ˆç‡ï¼Œå¯åœ¨äºŒè€…ä¹˜ç§¯ä¸å˜çš„æƒ…å†µä¸‹ï¼ŒåŠ å¤§ `per_device_train_batch_size` çš„å€¼ï¼Œä½†ä¹Ÿä¼šå¸¦æ¥æ›´å¤šçš„æ˜¾å­˜æ¶ˆè€—ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µé…Œæƒ…è°ƒæ•´ã€‚

å¦‚æœä½ æƒ³è¦[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](../README_en.md#load-the-model-locally)ï¼Œå¯ä»¥å°†ä¸Šè¿°ä»£ç ä¸­çš„ `THUDM/chatglm-6b` æ”¹ä¸ºä½ æœ¬åœ°çš„æ¨¡å‹è·¯å¾„ã€‚

### æ¨ç†

åœ¨ P-tuning v2 è®­ç»ƒæ—¶æ¨¡å‹åªä¿å­˜ PrefixEncoder éƒ¨åˆ†çš„å‚æ•°ï¼Œæ‰€ä»¥åœ¨æ¨ç†æ—¶éœ€è¦åŒæ—¶åŠ è½½åŸ ChatGLM-6B æ¨¡å‹ä»¥åŠ PrefixEncoder çš„æƒé‡ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
import ChatGLM6Bpkg

ChatGLM6Bpkg.ptuning.ptuning(
    do_predict=True,
    validation_file="AdvertiseGen/dev.json",
    test_file="AdvertiseGen/dev.json",
    overwrite_cache=True,
    prompt_column="content",
    response_column="summary",
    model_name_or_path="THUDM/chatglm-6b",
    ptuning_checkpoint="./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-100",
    output_dir="./output/adgen-chatglm-6b-pt-128-2e-2",
    overwrite_output_dir=True,
    max_source_length=64,
    max_target_length=64,
    per_device_eval_batch_size=1,
    predict_with_generate=True,
    pre_seq_len=128,
    quantization_bit=4
)
```

å…¶ä¸­ï¼Œ`model_name_or_path`æ˜¯åŸ ChatGLM-6B æ¨¡å‹çš„è·¯å¾„ï¼Œ`ptuning_checkpoint`æ˜¯ PrefixEncoder çš„æƒé‡è·¯å¾„ã€‚

ä»ç„¶å…¼å®¹æ—§ç‰ˆå…¨å‚ä¿å­˜çš„ Checkpointï¼Œåªéœ€è¦è®¾å®š `model_name_or_path`ä¸ºè®­ç»ƒåæ¨¡å‹çš„æƒé‡è·¯å¾„ï¼š

```python
model_name_or_path=$CHECKPOINT_PATH
```

è¯„æµ‹æŒ‡æ ‡ä¸ºä¸­æ–‡ Rouge score å’Œ BLEU-4ã€‚ç”Ÿæˆçš„ç»“æœä¿å­˜åœ¨
`./output/adgen-chatglm-6b-pt-128-2e-2/generated_predictions.txt`ã€‚

### æ¨¡å‹åŠ è½½

åœ¨ P-tuning v2 è®­ç»ƒåï¼Œå¯é€šè¿‡å¦‚ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹ï¼ˆåŸ ChatGLM-6B æ¨¡å‹ä»¥åŠ PrefixEncoder çš„æƒé‡ï¼‰ï¼š

```python
import ChatGLM6Bpkg

tokenizer, config, model = ChatGLM6Bpkg.ptuning.load_ptuning_checkpoint(
        model_name_or_path="THUDM/chatglm-6b",
        ptuning_checkpoint="./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-100",
        pre_seq_len=128,
        quantization_bit=4
    )
model = model.cuda()
model = model.eval()
ChatGLM6Bpkg.launch_web_demo(model=model, tokenizer=tokenizer)
```

## ChatGLM-6B ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€äº›ä½¿ç”¨ `web_demo.py` å¾—åˆ°çš„ç¤ºä¾‹æˆªå›¾ã€‚æ›´å¤š ChatGLM-6B çš„å¯èƒ½ï¼Œç­‰å¾…ä½ æ¥æ¢ç´¢å‘ç°ï¼

<details><summary><b>è‡ªæˆ‘è®¤çŸ¥</b></summary>

![](examples/self-introduction.png)

</details>

<details><summary><b>æçº²å†™ä½œ</b></summary>

![](examples/blog-outline.png)

</details>

<details><summary><b>æ–‡æ¡ˆå†™ä½œ</b></summary>

![](examples/ad-writing-2.png)

![](examples/comments-writing.png)

</details>

<details><summary><b>é‚®ä»¶å†™ä½œåŠ©æ‰‹</b></summary>

![](examples/email-writing-1.png)

![](examples/email-writing-2.png)

</details>

<details><summary><b>ä¿¡æ¯æŠ½å–</b></summary>

![](examples/information-extraction.png)

</details>

<details><summary><b>è§’è‰²æ‰®æ¼”</b></summary>

![](examples/role-play.png)

</details>

<details><summary><b>è¯„è®ºæ¯”è¾ƒ</b></summary>

![](examples/sport.png)

</details>

<details><summary><b>æ—…æ¸¸å‘å¯¼</b></summary>

![](examples/tour-guide.png)

</details>

## å±€é™æ€§

ç”±äº ChatGLM-6B çš„å°è§„æ¨¡ï¼Œå…¶èƒ½åŠ›ä»ç„¶æœ‰è®¸å¤šå±€é™æ€§ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬ç›®å‰å‘ç°çš„ä¸€äº›é—®é¢˜ï¼š

- æ¨¡å‹å®¹é‡è¾ƒå°ï¼š6B çš„å°å®¹é‡ï¼Œå†³å®šäº†å…¶ç›¸å¯¹è¾ƒå¼±çš„æ¨¡å‹è®°å¿†å’Œè¯­è¨€èƒ½åŠ›ã€‚åœ¨é¢å¯¹è®¸å¤šäº‹å®æ€§çŸ¥è¯†ä»»åŠ¡æ—¶ï¼ŒChatGLM-6B å¯èƒ½ä¼šç”Ÿæˆä¸æ­£ç¡®çš„ä¿¡æ¯ï¼›å®ƒä¹Ÿä¸æ“…é•¿é€»è¾‘ç±»é—®é¢˜ï¼ˆå¦‚æ•°å­¦ã€ç¼–ç¨‹ï¼‰çš„è§£ç­”ã€‚
    <details><summary><b>ç‚¹å‡»æŸ¥çœ‹ä¾‹å­</b></summary>

    ![](limitations/factual_error.png)

    ![](limitations/math_error.png)

    </details>

- äº§ç”Ÿæœ‰å®³è¯´æ˜æˆ–æœ‰åè§çš„å†…å®¹ï¼šChatGLM-6B åªæ˜¯ä¸€ä¸ªåˆæ­¥ä¸äººç±»æ„å›¾å¯¹é½çš„è¯­è¨€æ¨¡å‹ï¼Œå¯èƒ½ä¼šç”Ÿæˆæœ‰å®³ã€æœ‰åè§çš„å†…å®¹ã€‚ï¼ˆå†…å®¹å¯èƒ½å…·æœ‰å†’çŠ¯æ€§ï¼Œæ­¤å¤„ä¸å±•ç¤ºï¼‰

- è‹±æ–‡èƒ½åŠ›ä¸è¶³ï¼šChatGLM-6B è®­ç»ƒæ—¶ä½¿ç”¨çš„æŒ‡ç¤º/å›ç­”å¤§éƒ¨åˆ†éƒ½æ˜¯ä¸­æ–‡çš„ï¼Œä»…æœ‰æå°ä¸€éƒ¨åˆ†è‹±æ–‡å†…å®¹ã€‚å› æ­¤ï¼Œå¦‚æœè¾“å…¥è‹±æ–‡æŒ‡ç¤ºï¼Œå›å¤çš„è´¨é‡è¿œä¸å¦‚ä¸­æ–‡ï¼Œç”šè‡³ä¸ä¸­æ–‡æŒ‡ç¤ºä¸‹çš„å†…å®¹çŸ›ç›¾ï¼Œå¹¶ä¸”å‡ºç°ä¸­è‹±å¤¹æ‚çš„æƒ…å†µã€‚

- æ˜“è¢«è¯¯å¯¼ï¼Œå¯¹è¯èƒ½åŠ›è¾ƒå¼±ï¼šChatGLM-6B å¯¹è¯èƒ½åŠ›è¿˜æ¯”è¾ƒå¼±ï¼Œè€Œä¸” â€œè‡ªæˆ‘è®¤çŸ¥â€ å­˜åœ¨é—®é¢˜ï¼Œå¹¶å¾ˆå®¹æ˜“è¢«è¯¯å¯¼å¹¶äº§ç”Ÿé”™è¯¯çš„è¨€è®ºã€‚ä¾‹å¦‚å½“å‰ç‰ˆæœ¬çš„æ¨¡å‹åœ¨è¢«è¯¯å¯¼çš„æƒ…å†µä¸‹ï¼Œä¼šåœ¨è‡ªæˆ‘è®¤çŸ¥ä¸Šå‘ç”Ÿåå·®ã€‚
    <details><summary><b>ç‚¹å‡»æŸ¥çœ‹ä¾‹å­</b></summary>

    ![](limitations/self-confusion_google.jpg)

    ![](limitations/self-confusion_openai.jpg)

    ![](limitations/self-confusion_tencent.jpg)

    </details>

## åè®®

æœ¬é¡¹ç›®çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºï¼ŒChatGLM-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡

```
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```
```
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
```
