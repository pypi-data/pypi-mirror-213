{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb5819b",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c7dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this if your have an editable install\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af47053",
   "metadata": {},
   "source": [
    "### load your data\n",
    "\n",
    "For this quickstart we are going to be using a dataset that we prepared from [eli5](https://huggingface.co/datasets/eli5) dataset with the models response. The dataset is available in [huggingface](https://huggingface.co/datasets/explodinggradients/eli5-test).\n",
    "\n",
    "The dataset is of the following format\n",
    "| column name    | type      | description                                                                       |\n",
    "|----------------|-----------|-----------------------------------------------------------------------------------|\n",
    "| prompt         | str       | the prompt/question to answer                                                     |\n",
    "| context        | str       | context string that has any relevent priors the LLM needs to answer the questions |\n",
    "| references     | list[str] | reference documents the LLM can use to respond to the prompt                      |\n",
    "| ground_truth   | list[str] | accepted answers given by human annotators                                        |\n",
    "| generated_text | str       | the generated output from the LLM                                                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc9fb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jjmachan/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--eli5-test-217d92ce20e19249/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'prompt', 'ground_truth', 'references', 'generated_text'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "ds = load_dataset(\"explodinggradients/eli5-test\", split=\"test_eli5\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c0687",
   "metadata": {},
   "source": [
    "### choose the metrics\n",
    "\n",
    "ragas provides you with a wide range of metrics to evaluate the generated answers based on the latest research. You can see the entire list [here](https://github.com/explodinggradients/ragas#metrics). For this quickstart we will be using 3 from each type we support.\n",
    "1. `edit_ratio` - obtained by dividing the Levenshtein distance by sum of number of characters in generated text and ground truth.\n",
    "2. `bleu_score` - It measures precision by comparing  clipped n-grams in generated text to ground truth text.\n",
    "3. `bert_score` - measures the similarity between ground truth text answers and generated text using SBERT vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5abd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import edit_ratio, bleu_score, bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95d887",
   "metadata": {},
   "source": [
    "now we can initialize the `Evaluation` object. This will load your metrics and data and run the evaluation for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77c805d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import Evaluation\n",
    "\n",
    "e = Evaluation(\n",
    "    metrics=[bert_score, edit_ratio, bleu_score],\n",
    "    batched=False,\n",
    "    batch_size=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e879f51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjmachan/miniconda3/envs/bench/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jjmachan/miniconda3/envs/bench/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jjmachan/miniconda3/envs/bench/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# run it with .eval()\n",
    "result = e.eval(ds[\"ground_truth\"], ds[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbe76c",
   "metadata": {},
   "source": [
    "### analysing results\n",
    "\n",
    "The return `Result` object is used to analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "474c0aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'BERTScore_cosine'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.37552570906095206</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'edit_ratio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.41482407945510713</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'BLEU'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010848577619569451</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'BERTScore_cosine'\u001b[0m: \u001b[1;36m0.37552570906095206\u001b[0m, \u001b[32m'edit_ratio'\u001b[0m: \u001b[1;36m0.41482407945510713\u001b[0m, \u001b[32m'BLEU'\u001b[0m: \u001b[1;36m0.010848577619569451\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.pretty import pprint\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07bbec",
   "metadata": {},
   "source": [
    "you can access individual metric results via `result['<name>']`. it also has a `.describe()` function to show the distribution of the results and you can access the individual score from `.scores` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8c51b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BERTScore_cosine</th>\n",
       "      <th>edit_ratio</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.375526</td>\n",
       "      <td>0.414824</td>\n",
       "      <td>1.084858e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.212339</td>\n",
       "      <td>0.399876</td>\n",
       "      <td>3.489775e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.332697</td>\n",
       "      <td>0.429187</td>\n",
       "      <td>4.318061e-79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.532642</td>\n",
       "      <td>0.449509</td>\n",
       "      <td>1.525948e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.102182</td>\n",
       "      <td>4.029193e-232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.910680</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>1.506915e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.207559</td>\n",
       "      <td>0.058072</td>\n",
       "      <td>2.343307e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BERTScore_cosine  edit_ratio           BLEU\n",
       "mean          0.375526    0.414824   1.084858e-02\n",
       "25%           0.212339    0.399876  3.489775e-155\n",
       "50%           0.332697    0.429187   4.318061e-79\n",
       "75%           0.532642    0.449509   1.525948e-05\n",
       "min           0.007017    0.102182  4.029193e-232\n",
       "max           0.910680    0.572917   1.506915e-01\n",
       "std           0.207559    0.058072   2.343307e-02"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "# view with pandas\n",
    "df = DataFrame(result.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "421c60ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['BERTScore_cosine', 'edit_ratio', 'BLEU'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
