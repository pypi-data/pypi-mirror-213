{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c1732-2e44-4961-b74d-c0165ac142ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing with https://www.bambooweekly.com/p/bw-18-world-population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad1ad18-02b7-49aa-8a2f-7cb1cc0cf613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from buckaroo.buckaroo_widget import BuckarooWidget, disable, enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e4ae5-4c76-4c0f-ad5a-f6e42d3406b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /Users/paddy/Downloads/WPP2022_Demographic_Indicators_Medium.zip ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcf6e9-c984-4184-8301-a7102b5d201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip WPP2022_Demographic_Indicators_Medium.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2d577-200b-4e29-a7c9-474edfca3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_idxs(ser):\n",
    "    outlier_idxs = []\n",
    "    try:\n",
    "        idxs = ser.sort_values().index\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        idxs = ser.index\n",
    "    outlier_idxs.extend(idxs[:5])\n",
    "    outlier_idxs.extend(idxs[-5:])\n",
    "    return outlier_idxs\n",
    "\n",
    "def sample(df, sample_size=500, include_outliers=True):\n",
    "    if len(df) <= sample_size:\n",
    "        return df\n",
    "    sdf = df.sample(np.min([sample_size, len(df)]))\n",
    "    if include_outliers:\n",
    "        outlier_idxs = []\n",
    "        for col in df.columns:\n",
    "            outlier_idxs.extend(get_outlier_idxs(df[col]) )\n",
    "        outlier_idxs.extend(sdf.index)\n",
    "        uniq_idx = np.unique(outlier_idxs)\n",
    "        return df.iloc[uniq_idx]\n",
    "    return sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee947db-1c8f-49ff-bcde-da02034e4365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/jfr8ccjx09g2j_18ty_t3xnh0000gn/T/ipykernel_83712/944981008.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('WPP2022_Demographic_Indicators_Medium.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<' not supported between instances of 'float' and 'str'\n",
      "error reordering columns cannot reindex on an axis with duplicate labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paddy/buckaroo/buckaroo/summary_stats.py:148: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  sdf.loc['is_duplicate':, duplicate_cols] = -5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e945ea150c14c85b52c39601701fdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BuckarooWidget(commandConfig={'argspecs': {'dropcol': [None], 'to_datetime': [None], 'safeint': [None], 'fillnâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('WPP2022_Demographic_Indicators_Medium.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a05d2c-5728-407d-bed0-349673049056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buckaroo.summary_stats import reorder_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f882d-15d7-48ac-8f50-e3e51e0e07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2762b2-d0a5-4009-8fb9-dafc70f68398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probable_datetime(ser):\n",
    "    s_ser = ser.sample(np.min([len(ser), 500]))\n",
    "    try:\n",
    "        dt_ser = pd.to_datetime(s_ser)\n",
    "        #pd.to_datetime(1_00_000_000_000_000_000) == pd.to_datetime('1973-01-01') \n",
    "        if dt_ser.max() < pd.to_datetime('1973-01-01'):\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False\n",
    "#probable_datetime(df['start station name'])\n",
    "\n",
    "\n",
    "def summarize_string(ser):\n",
    "    l = len(ser)\n",
    "    val_counts = ser.value_counts()\n",
    "    distinct_count= len(val_counts)\n",
    "    nan_count = l - len(ser.dropna())\n",
    "    unique_count = len(val_counts[val_counts==1])\n",
    "    empty_count = val_counts.get('', 0)\n",
    "\n",
    "    #[pd.api.types.is_integer_dtype(df2[col]) for col in df2.columns]\n",
    "    #[pd.api.types.is_numeric_dtype(df2[col]) for col in df2.columns]\n",
    "\n",
    "    return dict(\n",
    "        dtype=ser.dtype,\n",
    "        length=l,\n",
    "        nan_count = nan_count,\n",
    "        distinct_count= distinct_count,\n",
    "        empty_count = empty_count,\n",
    "        empty_per = empty_count/l,\n",
    "        unique_per = unique_count/l,\n",
    "        nan_per = nan_count/l,\n",
    "        is_numeric=pd.api.types.is_numeric_dtype(ser),\n",
    "        is_integer=pd.api.types.is_integer_dtype(ser),\n",
    "        is_datetime=probable_datetime(ser),\n",
    "        mode=ser.mode().values[0])\n",
    "\n",
    "def summarize_numeric(ser):\n",
    "\n",
    "    num_stats = dict(\n",
    "        min=ser.min(),\n",
    "        max=ser.max(),\n",
    "        mean=ser.mean())\n",
    "    num_stats.update(summarize_string(ser))\n",
    "    return num_stats\n",
    "\n",
    "def summarize_column(ser):\n",
    "    if pd.api.types.is_numeric_dtype(ser.dtype):\n",
    "        return summarize_numeric(ser)\n",
    "    else:\n",
    "        return summarize_string(ser)\n",
    "\n",
    "def summarize_df(df):\n",
    "    summary_df = pd.DataFrame({col:summarize_column(df[col]) for col in df})\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "def set_when(df, cond_row_name, target_row_name, true_val, false_val):\n",
    "    true_row = df.loc[cond_row_name]\n",
    "    df.loc[target_row_name] = false_val\n",
    "    df.loc[target_row_name, true_row[true_row==True].index.values] = true_val\n",
    "    return df\n",
    "\n",
    "\n",
    "def without(arr, search_keys):\n",
    "    new_arr = []\n",
    "    for v in arr:\n",
    "        if v not in search_keys:\n",
    "            new_arr.append(v)\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def find_groupings(corr_pairs):\n",
    "    all_groupings = []\n",
    "    for key, other_key_list in corr_pairs.items():\n",
    "        ab = other_key_list.copy()\n",
    "        ab.append(key)\n",
    "        all_groupings.append(set(ab))\n",
    "    return np.unique(all_groupings)\n",
    "\n",
    "def reorder_columns(df):\n",
    "    tdf_stats = summarize_df(df)\n",
    "    cpd = get_cor_pair_dict(df, tdf_stats)\n",
    "    col_order = order_columns(tdf_stats, cpd)\n",
    "    return df[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d057a-2cfb-4655-82c1-28012fe02477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_categorical(ser):\n",
    "    ser_uniq = ser.dropna().unique()\n",
    "    name_to_idx = {name:idx for idx, name in enumerate(ser_uniq)}\n",
    "    #needs to be vectorized\n",
    "    num_categorical = ser.dropna().apply(lambda x:name_to_idx[x])\n",
    "    return num_categorical\n",
    "#make_num_categorical(df['Notes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24875ac9-bb33-4a1b-a015-f1ab1221f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cor_pair_dict(df, summary_stats):\n",
    "    #we need to remove columns with only nans or a single value, they mess up corr()\n",
    "\n",
    "    #this needs to be vectorized\n",
    "    corrable_cols = [col for col in df if summary_stats[col]['distinct_count'] > 1]\n",
    "    #print(\"corrable_cols\", corrable_cols)\n",
    "    #num_df =  pd.DataFrame({col:numerize_column(df[col]) for col in corrable_cols})\n",
    "\n",
    "    for col in df.columns:\n",
    "        make_num_categorical(df[col])\n",
    "    num_df =  pd.DataFrame({col:make_num_categorical(df[col]) for col in corrable_cols})\n",
    "\n",
    "    corr_df = num_df.corr()\n",
    "    high_corr_df = corr_df[corr_df > 0.99]\n",
    "    cor_dict = {}\n",
    "    for col in high_corr_df.columns:\n",
    "        #columns with high correlation that aren't the column itself\n",
    "        other_cor_cols = high_corr_df[col].dropna().drop(col)\n",
    "        cor_cols = other_cor_cols.index.values\n",
    "        if len(cor_cols) > 0:\n",
    "            cor_dict[col] = cor_cols.tolist()\n",
    "    return cor_dict\n",
    "\n",
    "\n",
    "def order_groupings(grps, ranked_cols):\n",
    "    first_cols, rest_cols = [], []\n",
    "    for col in ranked_cols:\n",
    "        for grp in grps:\n",
    "            if col in grp:\n",
    "                first_cols.append(col)\n",
    "                rest_cols.extend(list(without(grp, [col])))\n",
    "                grps = without(grps, [grp])\n",
    "    return first_cols, rest_cols\n",
    "\n",
    "def order_columns(summary_stats_df, corr_pair_dict):\n",
    "    sdf = summary_stats_df.copy()\n",
    "    sdf.loc['one_distinct'] = 0\n",
    "\n",
    "    only_ones = (sdf.loc['distinct_count'] <= 1)\n",
    "    sdf.loc['one_distinct', only_ones[only_ones==True].index.values] = -20\n",
    "    \n",
    "    sdf.loc['first_col'] = 0\n",
    "    sdf.loc['is_duplicate'] = 0\n",
    "    set_when(sdf, 'is_datetime', 'datetime_score', 11, 0)\n",
    "    \n",
    "    set_when(sdf, 'is_integer', 'grouping_score_integer', -3, 0)\n",
    "    set_when(sdf, 'is_numeric', 'grouping_score_numeric', -3, 5)\n",
    "    grouping_col_scores = sdf.loc[['grouping_score_integer', 'grouping_score_numeric']].sum()\n",
    "    duplicate_col_rankings = grouping_col_scores.sort_values().index[::-1].values\n",
    "\n",
    "    groupings = find_groupings(corr_pair_dict)\n",
    "    first_cols, duplicate_cols = order_groupings(groupings, duplicate_col_rankings)\n",
    "    \n",
    "    sdf.loc['first_col':, first_cols] = 5\n",
    "    \n",
    "    print(first_cols)\n",
    "    print(duplicate_cols)\n",
    "    sdf.loc['is_duplicate':, duplicate_cols] = -5\n",
    "    \n",
    "    col_scores = sdf.loc[['one_distinct', 'first_col', 'datetime_score', 'is_duplicate']].sum()\n",
    "    return col_scores.sort_values().index.values[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d57afa-dde9-4f0d-93ef-cb480016d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = summarize_df(df)\n",
    "cpd = get_cor_pair_dict(df, sdf)\n",
    "cpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549ceaa-b85e-4f8c-bc8c-f795ddca0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841e64d-19cb-47aa-8203-ba5e0b5aae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64583bd1-ecc1-4c6b-8c18-a929bc9e3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = ['ISO2_code', 'LocID', 'SortOrder', 'Location', 'TPopulation1Jan', 'Q0060Male', 'TPopulation1July', 'Q1560', 'ISO2_code', 'Q0060', 'SDMX_code', 'Location', 'Q0060Female', 'TPopulation1Jan', 'Q0060Male', 'ISO2_code', 'SDMX_code', 'Location', 'TPopulation1Jan', 'Q0060Male', 'TPopulation1July', 'Q1560', 'ISO2_code', 'Q0060', 'SDMX_code', 'Location', 'Q0060Female', 'TPopulation1Jan', 'Q1560Male', 'Q0060Male', 'TPopulation1July', 'Q1560', 'ISO2_code', 'Q0060', 'SDMX_code', 'Location', 'TPopulation1Jan', 'TPopulation1July', 'TPopulationFemale1July', 'ISO2_code', 'SDMX_code', 'Location', 'TPopulation1Jan', 'Q1560Male', 'Q0060Male', 'TPopulation1July', 'ISO2_code', 'SDMX_code', 'Location', 'Q0060Female', 'TPopulation1Jan', 'Q1560Male', 'Q0060Male', 'TPopulation1July', 'Q1560', 'TPopulationMale1July', 'ISO2_code', 'Q0060', 'SDMX_code', 'TPopulationFemale1July', 'Q0060Female', 'Location', 'PopDensity', 'TPopulation1Jan', 'Q1550Male', 'Q1560Male', 'Q0060Male', 'TPopulation1July', 'Q1560', 'TPopulationMale1July', 'ISO2_code', 'Q0060', 'SDMX_code', 'Q1560Female', 'TPopulationFemale1July', 'ISO2_code', 'ParentID', 'Location', 'Q0060Female', 'TPopulation1Jan', 'Q1550Male', 'Q1560Male', 'Q0060Male', 'TPopulation1July', 'Q1560', 'TPopulationMale1July', 'ISO2_code', 'Q0060', 'SDMX_code', 'Q1560Female', 'TPopulationFemale1July', 'ISO2_code', 'LocID', 'SortOrder', 'SortOrder', 'Location', 'Q0060Female', 'TPopulation1Jan', 'Q1560Male', 'ParentID', 'Q0060Male', 'TPopulation1July', 'Q1560', 'TPopulationFemale1July', 'ISO2_code', 'Q0060', 'LocID', 'SDMX_code', 'Location', 'TPopulation1Jan', 'Q1560Male', 'Q0060Male', 'TPopulation1July', 'Q1560', 'TPopulationMale1July', 'ISO2_code', 'Q0060', 'SDMX_code', 'TPopulationFemale1July', 'LocTypeID', 'Q1550Male', 'SDMX_code', 'PopDensity', 'TPopulation1Jan', 'TPopulation1July', 'TPopulationMale1July', 'SDMX_code', 'SDMX_code', 'Q1560Female', 'DoublingTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb176ee3-d9b7-4d3e-828d-c8c312212e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dups),len(np.unique(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97893ef6-5e21-4899-a742-62e6c387368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b983b17-9c39-42ae-8cb2-cf0503d2a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bc23c-45bc-4a2a-901a-b0ab39429f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d94d7-9f9a-4083-828a-f9d3aeb2c0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
