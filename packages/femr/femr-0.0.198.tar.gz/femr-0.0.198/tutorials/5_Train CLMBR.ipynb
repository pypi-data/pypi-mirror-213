{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train CLMBR\n",
    "\n",
    "This tutorial walks through the various steps to train a CLMBR model.\n",
    "\n",
    "Note that CLMBR requires the gpu enabled version of FEMR. See the [README](https://github.com/som-shahlab/femr#how-to-install-femr-with-cuda-support) for the relevant instructions.\n",
    "\n",
    "Training CLMBR is a three step process:\n",
    "\n",
    "- Generating a dictionary\n",
    "- Creating batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_5'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banned 0 out of 4523\n",
      "Got age statistics ... {\"mean\":834488.8237272064,\"std\":1516971.4691312013}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "EXTRACT_LOCATION = \"input/extract\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The first step of training CLMBR is creating a dictionary, that helps map codes to integers that can be used within a neural network.\n",
    "\"\"\"\n",
    "\n",
    "DICTIONARY_PATH = os.path.join(TARGET_DIR, \"dictionary\")\n",
    "os.system(f\"clmbr_create_dictionary {DICTIONARY_PATH} --data_path {EXTRACT_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 15:09:02,240 [MainThread  ] [INFO ]  Preparing batches with Namespace(directory='trash/tutorial_5/clmbr_batches', data_path='input/extract', dictionary_path='trash/tutorial_5/dictionary', task='clmbr', transformer_vocab_size=2048, clmbr_survival_dictionary_path=None, labeled_patients_path=None, is_hierarchical=False, seed=97, val_start=70, test_start=85, batch_size=16384, note_embedding_data=None, limit_to_patients_file=None, limit_before_date=None, num_clmbr_tasks=8192)\n",
      "2023-05-30 15:09:02,295 [MainThread  ] [INFO ]  Wrote config ...\n",
      "2023-05-30 15:09:02,295 [MainThread  ] [INFO ]  Starting to load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 15:09:02,765 [MainThread  ] [INFO ]  Loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 15:09:03,003 [MainThread  ] [INFO ]  Number of train patients 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The second step of training CLMBR is to prepare the batches that will actually get fed into the neural network.\n",
    "\"\"\"\n",
    "\n",
    "CLMBR_BATCHES = os.path.join(TARGET_DIR, \"clmbr_batches\")\n",
    "\n",
    "os.system(\n",
    "    f\"clmbr_create_batches {CLMBR_BATCHES} --data_path {EXTRACT_LOCATION} --dictionary {DICTIONARY_PATH} --task clmbr --transformer_vocab_size 2048\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 15:09:04,484 [MainThread  ] [INFO ]  Training model with Namespace(directory='trash/tutorial_5/clmbr_model', data_path='input/extract', batches_path='trash/tutorial_5/clmbr_batches', learning_rate=0.0001, rotary_type='per_head', clmbr_survival_dim=None, num_batch_threads=3, start_from_checkpoint=None, freeze_weights=False, token_dropout=0, internal_dropout=0, weight_decay=0, max_iter=10, hidden_size=256, intermediate_size=256, n_heads=4, n_layers=1, attention_width=512, dev_batches_path=None, linear_probe_head=None, early_stopping_window_steps=None)\n",
      "2023-05-30 15:09:04,496 [MainThread  ] [INFO ]  Got config {'data_path': 'input/extract', 'batch_info_path': 'trash/tutorial_5/clmbr_batches/batch_info.msgpack', 'seed': 97, 'task': {'type': 'clmbr', 'vocab_size': 8192}, 'transformer': {'vocab_size': 2048, 'hidden_size': 256, 'intermediate_size': 256, 'n_heads': 4, 'n_layers': 1, 'rotary': 'per_head', 'attention_width': 496, 'internal_dropout': 0, 'is_hierarchical': False, 'note_embedding_data': None}, 'learning_rate': 0.0001, 'max_grad_norm': 1.0, 'weight_decay': 0, 'n_epochs': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 15:09:04,686 [MainThread  ] [INFO ]  Loaded batches 1 1\n",
      "2023-05-30 15:09:05.861837: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:7: failed initializing StreamExecutor for CUDA device ordinal 7: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 34089730048\n",
      "2023-05-30 15:09:05.868774: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:6: failed initializing StreamExecutor for CUDA device ordinal 6: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 34089730048\n",
      "2023-05-30 15:09:05.875473: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:5: failed initializing StreamExecutor for CUDA device ordinal 5: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 34089730048\n",
      "2023-05-30 15:09:05.879719: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:4: failed initializing StreamExecutor for CUDA device ordinal 4: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 34089730048\n",
      "2023-05-30 15:09:06,475 [MainThread  ] [INFO ]  Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Host Interpreter CUDA\n",
      "2023-05-30 15:09:06,475 [MainThread  ] [INFO ]  Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-05-30 15:09:06,475 [MainThread  ] [INFO ]  Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "2023-05-30 15:09:13,131 [MainThread  ] [INFO ]  Got dummy batch {'num_indices': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'num_patients': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'offsets': ((512,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'patient_ids': ((512,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'task': {'labels': ((4096,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))}, 'transformer': {'ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'label_indices': ((4096,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'length': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'normalized_ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'tokens': ((16384,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'valid_tokens': ((16384,), dtype('bool'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))}}\n",
      "2023-05-30 15:09:13,234 [MainThread  ] [INFO ]  Transformed the model function\n",
      "2023-05-30 15:09:14,364 [MainThread  ] [INFO ]  Done initing {'EHRTransformer/~/CLMBRTask/~/linear': {'b': ((8192,), dtype('float32')), 'w': ((256, 8192), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': ((2048, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': ((1024,), dtype('float32')), 'w': ((256, 1024), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': ((256,), dtype('float32')), 'w': ((512, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': ((256,), dtype('float32'))}}\n",
      "2023-05-30 15:09:14,364 [MainThread  ] [INFO ]  Total params 3024896\n",
      "2023-05-30 15:09:14,365 [MainThread  ] [INFO ]  total steps 100 num train batches 1\n",
      "2023-05-30 15:09:14,365 [MainThread  ] [INFO ]  Applying decay mask {'EHRTransformer/~/CLMBRTask/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': False}}\n",
      "2023-05-30 15:09:14,365 [MainThread  ] [INFO ]  Using weight decay 0\n",
      "2023-05-30 15:09:14,977 [MainThread  ] [INFO ]  Starting loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=array(0, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:17,229 [MainThread  ] [INFO ]  Starting train loss {'loss': 9.364985466003418, 'c_statistic': -9.364985466003418}\n",
      "2023-05-30 15:09:17,879 [MainThread  ] [INFO ]  Starting dev loss {'loss': 9.403003692626953, 'c_statistic': -9.403003692626953}\n",
      "Working with seed 9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 15:09:18,595 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=array(0, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:18,615 [MainThread  ] [INFO ]  Train loss {'loss': 9.364985466003418, 'c_statistic': -9.364985466003418}\n",
      "2023-05-30 15:09:18,625 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403003692626953, 'c_statistic': -9.403003692626953}\n",
      "2023-05-30 15:09:18,768 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "/home/ethanid/miniconda3/envs/femr_develop2/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(uint32[4096]), ShapedArray(float32[16384]), ShapedArray(uint32[4096]), ShapedArray(int32[], weak_type=True), ShapedArray(uint32[16384]), ShapedArray(bool[16384]).\n",
      "See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\n",
      "  warnings.warn(f\"Some donated buffers were not usable: {', '.join(unused_donations)}.\\n{msg}\")\n",
      "2023-05-30 15:09:21,375 [MainThread  ] [INFO ]  [Step 0]\n",
      "2023-05-30 15:09:21,391 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(2, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,629 [MainThread  ] [INFO ]  Train loss {'loss': 9.365042686462402, 'c_statistic': -9.365042686462402}\n",
      "2023-05-30 15:09:21,637 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403146743774414, 'c_statistic': -9.403146743774414}\n",
      "2023-05-30 15:09:21,638 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,641 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(3, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,657 [MainThread  ] [INFO ]  Train loss {'loss': 9.365055084228516, 'c_statistic': -9.365055084228516}\n",
      "2023-05-30 15:09:21,666 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403162956237793, 'c_statistic': -9.403162956237793}\n",
      "2023-05-30 15:09:21,667 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,672 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(4, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,688 [MainThread  ] [INFO ]  Train loss {'loss': 9.365017890930176, 'c_statistic': -9.365017890930176}\n",
      "2023-05-30 15:09:21,697 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403162956237793, 'c_statistic': -9.403162956237793}\n",
      "2023-05-30 15:09:21,697 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,700 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(5, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,715 [MainThread  ] [INFO ]  Train loss {'loss': 9.364958763122559, 'c_statistic': -9.364958763122559}\n",
      "2023-05-30 15:09:21,720 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403115272521973, 'c_statistic': -9.403115272521973}\n",
      "2023-05-30 15:09:21,720 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,723 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(6, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,738 [MainThread  ] [INFO ]  Train loss {'loss': 9.36495304107666, 'c_statistic': -9.36495304107666}\n",
      "2023-05-30 15:09:21,744 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403003692626953, 'c_statistic': -9.403003692626953}\n",
      "2023-05-30 15:09:21,744 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,747 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(7, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,763 [MainThread  ] [INFO ]  Train loss {'loss': 9.3649263381958, 'c_statistic': -9.3649263381958}\n",
      "2023-05-30 15:09:21,768 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403178215026855, 'c_statistic': -9.403178215026855}\n",
      "2023-05-30 15:09:21,769 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,771 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(8, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,787 [MainThread  ] [INFO ]  Train loss {'loss': 9.364903450012207, 'c_statistic': -9.364903450012207}\n",
      "2023-05-30 15:09:21,792 [MainThread  ] [INFO ]  Dev loss {'loss': 9.40305233001709, 'c_statistic': -9.40305233001709}\n",
      "2023-05-30 15:09:21,793 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,795 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(9, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,811 [MainThread  ] [INFO ]  Train loss {'loss': 9.364849090576172, 'c_statistic': -9.364849090576172}\n",
      "2023-05-30 15:09:21,816 [MainThread  ] [INFO ]  Dev loss {'loss': 9.40294075012207, 'c_statistic': -9.40294075012207}\n",
      "2023-05-30 15:09:21,845 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,849 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(10, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,865 [MainThread  ] [INFO ]  Train loss {'loss': 9.364794731140137, 'c_statistic': -9.364794731140137}\n",
      "2023-05-30 15:09:21,870 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403194427490234, 'c_statistic': -9.403194427490234}\n",
      "2023-05-30 15:09:21,870 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-30 15:09:21,873 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(11, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-30 15:09:21,889 [MainThread  ] [INFO ]  Train loss {'loss': 9.364797592163086, 'c_statistic': -9.364797592163086}\n",
      "2023-05-30 15:09:21,894 [MainThread  ] [INFO ]  Dev loss {'loss': 9.403162956237793, 'c_statistic': -9.403162956237793}\n",
      "2023-05-30 15:09:21,895 [MainThread  ] [INFO ]  Stopping due to max iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the transformer ... (16384,) (4096,)\n",
      "WITHOUT AGE\n",
      "WITHOUT AGE\n",
      "Compiling the transformer ... (16384,) (4096,)\n",
      "WITHOUT AGE\n",
      "Compiling the transformer ... (16384,) (256,)\n",
      "WITHOUT AGE\n",
      "Compiling the transformer ... (16384,) (4096,)\n",
      "WITHOUT AGE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the batches, it is now possible to train CLMBR. By default it will train for 100 epochs, with early stopping.\n",
    "\"\"\"\n",
    "\n",
    "MODEL_PATH = os.path.join(TARGET_DIR, \"clmbr_model\")\n",
    "\n",
    "\n",
    "assert 0 == os.system(\n",
    "    f\"clmbr_train_model {MODEL_PATH} --data_path {EXTRACT_LOCATION} --batches_path {CLMBR_BATCHES} --learning_rate 1e-4 --rotary_type per_head --num_batch_threads 3 --max_iter 10 --n_layers 1 --hidden_size 256 --n_heads 4 --intermediate_size 256\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
